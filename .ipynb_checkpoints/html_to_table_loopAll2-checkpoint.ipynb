{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "    - Some docs are not in English, even though this criterion was always specified for each database search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module collates HTML exports of Proquest search result records in the parent directory and converts the collection into a table in a time-stamped Excel workbook.\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from lxml import html\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "# obtain data files\n",
    "files = list()\n",
    "for filename in Path('.').glob('**/?[Pro]*.html'):\n",
    "    files.append(filename) \n",
    "files\n",
    "\n",
    "# import list of features to filter docs\n",
    "ftname = r\"./features.txt\"\n",
    "with open(ftname, 'r', encoding='utf-8') as fh:\n",
    "    ftrs = list()\n",
    "    for line in fh:\n",
    "        line = line.replace(\"\\n\",\"\").replace(\":\",\": \")\n",
    "        ftrs.append(line)\n",
    "fin_tab = list()\n",
    "for i_ft, elem_ft in enumerate(ftrs):\n",
    "    #iterate through each html source file\n",
    "    for i_dt, elem_dt in enumerate(files):\n",
    "        HtmlFile = open(elem_dt, 'r', encoding='utf-8')\n",
    "        page = HtmlFile.read()\n",
    "        # generate documents by splitting source file\n",
    "        docs = page.split(\n",
    "            \"\"\"<div style=\"margin-bottom:20px;\"\"\"\n",
    "            \"\"\"border-bottom:2px solid #ccc;padding-bottom:5px\">\"\"\")\n",
    "        # delete table of contents and cover page\n",
    "        docs.pop(0)     \n",
    "        # iterate through each document \n",
    "        for i_dc, elem_dc in enumerate(docs):\n",
    "            # remove all simple paragraph tags\n",
    "            x = elem_dc #.replace(\"<p>\",\"\").replace(\"</p>\",\"\")\n",
    "            div = html.fromstring(x) # read text as html\n",
    "            # get list of available features:\n",
    "            doc_ftrs = [x.text for x in (div.xpath('//strong'))]\n",
    "            # get index of iteration feature in available features list\n",
    "            doc_ftrs\n",
    "            # append html text for iteration feature to final table\n",
    "            try:\n",
    "                fin_tab.append(\n",
    "                    div.xpath('//strong')[doc_ftrs.index(ftrs[i_ft])]\n",
    "                    .xpath(\"./following::text()[1]\")[0])\n",
    "            except:\n",
    "                fin_tab.append(\"NA\") \n",
    "    print(\"feature:\", i_ft, end=\"  \")\n",
    "print(\"original files:\", i_dt+1)\n",
    "\n",
    "# alternative/additional (unnamed) features\n",
    "for i_dt, elem_dt in enumerate(files):\n",
    "    HtmlFile = open(elem_dt, 'r', encoding='utf-8')\n",
    "    page = HtmlFile.read()\n",
    "    # generate documents by splitting source file\n",
    "    docs = page.split(\n",
    "        \"\"\"<div style=\"margin-bottom:20px;\"\"\"\n",
    "        \"\"\"border-bottom:2px solid #ccc;padding-bottom:5px\">\"\"\")\n",
    "    # delete table of contents and cover page\n",
    "    docs.pop(0) \n",
    "    for i_dc, elem_dc in enumerate(docs):\n",
    "        # remove all simple paragraph tags\n",
    "        soup = BeautifulSoup(elem_dc)\n",
    "        h4s = soup.find_all(\"text\")\n",
    "        try:\n",
    "            fin_tab.append(h4s[0].text)\n",
    "        except:\n",
    "            fin_tab.append(\"NA\")\n",
    "\n",
    "# alternative/additional (unnamed) features: Doc Titles\n",
    "for i_dt, elem_dt in enumerate(files):\n",
    "    HtmlFile = open(elem_dt, 'r', encoding='utf-8')\n",
    "    page = HtmlFile.read()\n",
    "    # generate documents by splitting source file\n",
    "    docs = page.split(\n",
    "        \"\"\"<div style=\"margin-bottom:20px;\"\"\"\n",
    "        \"\"\"border-bottom:2px solid #ccc;padding-bottom:5px\">\"\"\")\n",
    "    # delete table of contents and cover page\n",
    "    docs.pop(0) \n",
    "    for i_dc, elem_dc in enumerate(docs):\n",
    "        # remove all simple paragraph tags\n",
    "        soup = BeautifulSoup(elem_dc)\n",
    "        h4s = soup.find_all(\"p\")\n",
    "        try:\n",
    "            fin_tab.append(h4s[1].text)\n",
    "        except:\n",
    "            fin_tab.append(\"NA\")\n",
    "            \n",
    "# append list of alternative/additional (unnamed) features for output\n",
    "ftname = r\"./featuresx_fintab.txt\"\n",
    "with open(ftname, 'r', encoding='utf-8') as fh:\n",
    "    for line in fh:\n",
    "        line = line.replace(\"\\n\",\"\").replace(\":\",\": \")\n",
    "        ftrs.append(line)\n",
    "        \n",
    "# set safe dataframe names\n",
    "ftrs = [x.replace(\": \",\"\").replace(\" \",\"_\") for x in ftrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: 787\n"
     ]
    }
   ],
   "source": [
    "# export to excel       \n",
    "n_obs = int((len(fin_tab)/len(ftrs)))\n",
    "fin_tab = chunks(fin_tab, n_obs)\n",
    "fin_tab = list(fin_tab)\n",
    "fin_tab[3][0:4]\n",
    "\n",
    "print(\"obs:\", n_obs)\n",
    "\n",
    "# Populate columns of a dataframe by feature\n",
    "df = pd.DataFrame(fin_tab[0], columns = [ftrs[0]])\n",
    "for i, elem in enumerate(ftrs):\n",
    "    df[ftrs[i]] = fin_tab[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProQuest_document_ID</th>\n",
       "      <th>Publication_info</th>\n",
       "      <th>Full_text</th>\n",
       "      <th>Publication_title</th>\n",
       "      <th>Publication_date</th>\n",
       "      <th>Country_of_publication</th>\n",
       "      <th>Source_type</th>\n",
       "      <th>Document_type</th>\n",
       "      <th>Location</th>\n",
       "      <th>Document_URL</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Publication_subject</th>\n",
       "      <th>Full_Text2</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>460864564</td>\n",
       "      <td>NA</td>\n",
       "      <td></td>\n",
       "      <td>BBC Monitoring Asia Pacific; London</td>\n",
       "      <td>Jan 3, 2008</td>\n",
       "      <td>United Kingdom, London</td>\n",
       "      <td>Wire Feeds</td>\n",
       "      <td>WIRE FEED</td>\n",
       "      <td>NA</td>\n",
       "      <td>https://login.proxy.lib.fsu.edu/login?url=http...</td>\n",
       "      <td>NA</td>\n",
       "      <td>Business And Economics, Political Science</td>\n",
       "      <td>Text of report in English by Taiwanese Cen...</td>\n",
       "      <td>Taiwan: Brazil suspends issuing visas to Taiwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>460624584</td>\n",
       "      <td>NA</td>\n",
       "      <td></td>\n",
       "      <td>BBC Monitoring Asia Pacific; London</td>\n",
       "      <td>Jan 4, 2008</td>\n",
       "      <td>United Kingdom, London</td>\n",
       "      <td>Wire Feeds</td>\n",
       "      <td>WIRE FEED</td>\n",
       "      <td>NA</td>\n",
       "      <td>https://login.proxy.lib.fsu.edu/login?url=http...</td>\n",
       "      <td>NA</td>\n",
       "      <td>Business And Economics, Political Science</td>\n",
       "      <td>Text of report in English by Taiwan News w...</td>\n",
       "      <td>Taiwan foreign ministry urges Brazil to lift t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2249408020</td>\n",
       "      <td>NA</td>\n",
       "      <td>\\nNot available.\\n</td>\n",
       "      <td>South Florida Sun Sentinel (2000-2011); Fort L...</td>\n",
       "      <td>Jan 6, 2008</td>\n",
       "      <td>United States, Fort Lauderdale, Florida</td>\n",
       "      <td>Historical Newspapers</td>\n",
       "      <td>News</td>\n",
       "      <td>NA</td>\n",
       "      <td>https://login.proxy.lib.fsu.edu/login?url=http...</td>\n",
       "      <td>NA</td>\n",
       "      <td>General Interest Periodicals--United States</td>\n",
       "      <td>NA</td>\n",
       "      <td>January 6, 2008 (Page 62 of 264)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264219488</td>\n",
       "      <td>NA</td>\n",
       "      <td>\\n          \\n</td>\n",
       "      <td>St. Petersburg Times; St. Petersburg, Fla.</td>\n",
       "      <td>Jan 6, 2008</td>\n",
       "      <td>United States, St. Petersburg, Fla.</td>\n",
       "      <td>Newspapers</td>\n",
       "      <td>NEWSPAPER</td>\n",
       "      <td>NA</td>\n",
       "      <td>https://login.proxy.lib.fsu.edu/login?url=http...</td>\n",
       "      <td>NA</td>\n",
       "      <td>General Interest Periodicals--United States</td>\n",
       "      <td>\\n\\n\\n\\n\\n   Travel implies going to a place, ...</td>\n",
       "      <td>AROUND OUR WORLD IN 365 DAYS Series: SPECIAL T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ProQuest_document_ID Publication_info                   Full_text  \\\n",
       "0            460864564               NA                               \n",
       "1            460624584               NA                               \n",
       "2           2249408020               NA          \\nNot available.\\n   \n",
       "3            264219488               NA  \\n          \\n               \n",
       "\n",
       "                                   Publication_title Publication_date  \\\n",
       "0                BBC Monitoring Asia Pacific; London      Jan 3, 2008   \n",
       "1                BBC Monitoring Asia Pacific; London      Jan 4, 2008   \n",
       "2  South Florida Sun Sentinel (2000-2011); Fort L...      Jan 6, 2008   \n",
       "3         St. Petersburg Times; St. Petersburg, Fla.      Jan 6, 2008   \n",
       "\n",
       "                    Country_of_publication            Source_type  \\\n",
       "0                   United Kingdom, London             Wire Feeds   \n",
       "1                   United Kingdom, London             Wire Feeds   \n",
       "2  United States, Fort Lauderdale, Florida  Historical Newspapers   \n",
       "3      United States, St. Petersburg, Fla.             Newspapers   \n",
       "\n",
       "  Document_type Location                                       Document_URL  \\\n",
       "0     WIRE FEED       NA  https://login.proxy.lib.fsu.edu/login?url=http...   \n",
       "1     WIRE FEED       NA  https://login.proxy.lib.fsu.edu/login?url=http...   \n",
       "2          News       NA  https://login.proxy.lib.fsu.edu/login?url=http...   \n",
       "3     NEWSPAPER       NA  https://login.proxy.lib.fsu.edu/login?url=http...   \n",
       "\n",
       "  Subject                          Publication_subject  \\\n",
       "0      NA    Business And Economics, Political Science   \n",
       "1      NA    Business And Economics, Political Science   \n",
       "2      NA  General Interest Periodicals--United States   \n",
       "3      NA  General Interest Periodicals--United States   \n",
       "\n",
       "                                          Full_Text2  \\\n",
       "0      Text of report in English by Taiwanese Cen...   \n",
       "1      Text of report in English by Taiwan News w...   \n",
       "2                                                 NA   \n",
       "3  \\n\\n\\n\\n\\n   Travel implies going to a place, ...   \n",
       "\n",
       "                                               Title  \n",
       "0  Taiwan: Brazil suspends issuing visas to Taiwa...  \n",
       "1  Taiwan foreign ministry urges Brazil to lift t...  \n",
       "2                   January 6, 2008 (Page 62 of 264)  \n",
       "3  AROUND OUR WORLD IN 365 DAYS Series: SPECIAL T...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_excel(\"proquest_data_\" \n",
    "            + str(datetime.datetime.now())[0:19].replace(\":\",\"_\") \n",
    "            + \".xlsx\")\n",
    "df.head(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('proquest_data_2019-07-17 01_43_00.xlsx')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain tabular data files\n",
    "files = list()\n",
    "for filename in Path('.').glob('**/?[Prq]*.xlsx'):\n",
    "    files.append(filename) \n",
    "latest_file = max(files, key=os.path.getctime)\n",
    "latest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(787, 14)\n"
     ]
    }
   ],
   "source": [
    "# set stop words\n",
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "# cleanup dataset\n",
    "df1 = pd.read_excel(latest_file)\n",
    "df1 = df1.drop(df1.columns[0], axis=1)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(519, 14)\n",
      "(444, 14)\n",
      "(443, 14)\n",
      "(141, 15)\n",
      "(141, 16)\n",
      "(137, 17)\n"
     ]
    }
   ],
   "source": [
    "# drop documents with type \"wire feeds\"\n",
    "df1 = df1[df1[\"Source_type\"] != \"Wire Feeds\"]\n",
    "print(df1.shape)\n",
    "# drop blank documents\n",
    "df1 = df1[df1[\"Full_Text2\"].notnull()]\n",
    "print(df1.shape)\n",
    "# strip document trailing and leading whitespace\n",
    "df1[\"Full_Text2\"] = df1[\"Full_Text2\"].str.strip()\n",
    "\n",
    "# drop \"caption only\" documents\n",
    "df1 = df1[df1[\"Full_Text2\"].str.lower().str.count(\n",
    "    \"caption text only\")==0]\n",
    "print(df1.shape)\n",
    "# keep documents that mention carnival more than once;\n",
    "# then save word count\n",
    "df1 = df1[df1[\"Full_Text2\"].str.lower().str.count(\n",
    "    \"carnival|carnaval|carnavale\")>1]\n",
    "df1[\"carnival_count\"] = df1[\"Full_Text2\"].str.lower().str.count(\n",
    "    \"carnival|carnaval|carnavale\")\n",
    "print(df1.shape)\n",
    "# keep documents with more than 300 characters;\n",
    "# save character count\n",
    "df1 = df1[df1[\"Full_Text2\"].str.lower().str.len()>300]\n",
    "df1[\"char_count\"] = df1[\"Full_Text2\"].str.lower().str.len()\n",
    "print(df1.shape)\n",
    "# create duplicate for comparison \n",
    "df1[\"doc\"] = df1[\"Full_Text2\"]\n",
    "\n",
    "# extricate non-english documents\n",
    "df_foreign = df1[df1[\"doc\"].str.lower().str.count(\n",
    "    \"algun|cosas|tener|algumas|coisas\")>=1]\n",
    "df_foreign.to_excel(\"foreign_lang_data_\" \n",
    "            + str(datetime.datetime.now())[0:19].replace(\":\",\"_\") \n",
    "            + \".xlsx\")\n",
    "df1 = df1[df1[\"doc\"].str.lower().str.count(\n",
    "    \"algun|cosas|tener|algumas|coisas\")<1]\n",
    "print(df1.shape)\n",
    "\n",
    "# Extract additional helpful features\n",
    "df1[\"Country\"] = df['Country_of_publication'].str.extract('^(.+?),')\n",
    "df1[\"Country\"] = df1[\"Country\"].str.replace(\"United Sta tes\",\"United States\")\n",
    "df1[\"Country\"] = df1[\"Country\"].str.replace(\"New Yor k\",\"United States\")\n",
    "df1[\"Country\"] = df1[\"Country\"].str.replace(\"London\",\"United Kingdom\")\n",
    "\n",
    "## Year\n",
    "df1['Publication_date'] = df1['Publication_date'].str.replace(\"201 8\",\"2018\")\n",
    "df1[\"Year\"] = df1['Publication_date'].str.extract('(\\d{4})')\n",
    "# df1[\"Year\"] = df1['Publication_date'].str.extract(',(.+)')\n",
    "\n",
    "\n",
    "# export R-ready dataset\n",
    "df1.to_excel(\"R_ready_data\" \n",
    "            + str(datetime.datetime.now())[0:19].replace(\":\",\"_\") \n",
    "            + \".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further pythonic pre-processing\n",
    "\n",
    "# remove numbers \n",
    "df1[\"doc\"] = [re.sub(r\"\\d+\", \"\", doc, flags=re.MULTILINE) for doc in df1[\"doc\"]]\n",
    "\n",
    "# remove URLS\n",
    "df1[\"doc\"] = [re.sub(r\"www\\S+\", \"\", doc, flags=re.MULTILINE) for doc in df1[\"doc\"]]\n",
    "\n",
    "# # remove punctuation, leaving apostrophied possessive \n",
    "# # and hyphenated words intact; make lowercase\n",
    "# df1[\"doc\"] = df1['doc'].apply(lambda x: \" \".join(\n",
    "#     [word.strip(string.punctuation) for word in x.split(\" \")]).strip()).str.lower()\n",
    "\n",
    "# remove punctuation; make lowercase\n",
    "df1[\"doc\"] = df1[\"doc\"].str.replace('[^\\w\\s]','').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 19)\n",
      "(137, 20)\n",
      "(137, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProQuest_document_ID</th>\n",
       "      <th>Publication_info</th>\n",
       "      <th>Full_text</th>\n",
       "      <th>Publication_title</th>\n",
       "      <th>Publication_date</th>\n",
       "      <th>Country_of_publication</th>\n",
       "      <th>Source_type</th>\n",
       "      <th>Document_type</th>\n",
       "      <th>Location</th>\n",
       "      <th>Document_URL</th>\n",
       "      <th>...</th>\n",
       "      <th>Publication_subject</th>\n",
       "      <th>Full_Text2</th>\n",
       "      <th>Title</th>\n",
       "      <th>carnival_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>doc</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>docl</th>\n",
       "      <th>docf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264219488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n          \\n</td>\n",
       "      <td>St. Petersburg Times; St. Petersburg, Fla.</td>\n",
       "      <td>Jan 6, 2008</td>\n",
       "      <td>United States, St. Petersburg, Fla.</td>\n",
       "      <td>Newspapers</td>\n",
       "      <td>NEWSPAPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://login.proxy.lib.fsu.edu/login?url=http...</td>\n",
       "      <td>...</td>\n",
       "      <td>General Interest Periodicals--United States</td>\n",
       "      <td>Travel implies going to a place, but some of t...</td>\n",
       "      <td>AROUND OUR WORLD IN 365 DAYS Series: SPECIAL T...</td>\n",
       "      <td>3</td>\n",
       "      <td>9786</td>\n",
       "      <td>travel implies going place exciting trips even...</td>\n",
       "      <td>United States</td>\n",
       "      <td>2008</td>\n",
       "      <td>travel implies going place exciting trip event...</td>\n",
       "      <td>travel going place exciting trip event would i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>284122990</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Orlando Sentinel; Orlando, Fla.</td>\n",
       "      <td>Jan 13, 2008</td>\n",
       "      <td>United States, Orlando, Fla.</td>\n",
       "      <td>Newspapers</td>\n",
       "      <td>NEWSPAPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://login.proxy.lib.fsu.edu/login?url=http...</td>\n",
       "      <td>...</td>\n",
       "      <td>General Interest Periodicals--United States</td>\n",
       "      <td>RIO DE JANEIRO, Brazil -- Everyone knows Rio d...</td>\n",
       "      <td>Find your fun at Brazil's carnival; Feel free ...</td>\n",
       "      <td>14</td>\n",
       "      <td>3825</td>\n",
       "      <td>rio de janeiro brazil everyone knows rio de ja...</td>\n",
       "      <td>United States</td>\n",
       "      <td>2008</td>\n",
       "      <td>rio de janeiro brazil everyone know rio de jan...</td>\n",
       "      <td>janeiro everyone know janeiros supposed greate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProQuest_document_ID Publication_info                   Full_text  \\\n",
       "3             264219488              NaN  \\n          \\n               \n",
       "12            284122990              NaN                               \n",
       "\n",
       "                             Publication_title Publication_date  \\\n",
       "3   St. Petersburg Times; St. Petersburg, Fla.      Jan 6, 2008   \n",
       "12             Orlando Sentinel; Orlando, Fla.     Jan 13, 2008   \n",
       "\n",
       "                 Country_of_publication Source_type Document_type Location  \\\n",
       "3   United States, St. Petersburg, Fla.  Newspapers     NEWSPAPER      NaN   \n",
       "12         United States, Orlando, Fla.  Newspapers     NEWSPAPER      NaN   \n",
       "\n",
       "                                         Document_URL  ...  \\\n",
       "3   https://login.proxy.lib.fsu.edu/login?url=http...  ...   \n",
       "12  https://login.proxy.lib.fsu.edu/login?url=http...  ...   \n",
       "\n",
       "                            Publication_subject  \\\n",
       "3   General Interest Periodicals--United States   \n",
       "12  General Interest Periodicals--United States   \n",
       "\n",
       "                                           Full_Text2  \\\n",
       "3   Travel implies going to a place, but some of t...   \n",
       "12  RIO DE JANEIRO, Brazil -- Everyone knows Rio d...   \n",
       "\n",
       "                                                Title carnival_count  \\\n",
       "3   AROUND OUR WORLD IN 365 DAYS Series: SPECIAL T...              3   \n",
       "12  Find your fun at Brazil's carnival; Feel free ...             14   \n",
       "\n",
       "    char_count                                                doc  \\\n",
       "3         9786  travel implies going place exciting trips even...   \n",
       "12        3825  rio de janeiro brazil everyone knows rio de ja...   \n",
       "\n",
       "          Country  Year                                               docl  \\\n",
       "3   United States  2008  travel implies going place exciting trip event...   \n",
       "12  United States  2008  rio de janeiro brazil everyone know rio de jan...   \n",
       "\n",
       "                                                 docf  \n",
       "3   travel going place exciting trip event would i...  \n",
       "12  janeiro everyone know janeiros supposed greate...  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop stop words\n",
    "df1[\"doc\"] = df1['doc'].apply(lambda x: ' '.join(\n",
    "    [word for word in x.split() if word not in (en_stops)]))\n",
    "print(df1.shape)\n",
    "\n",
    "# lemmatize words in each document\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "df1[\"docl\"] = df1[\"doc\"].apply(lambda x: ' '.join(\n",
    "    [wordnet_lemmatizer.lemmatize(word) for word in nltk.word_tokenize(x)]))\n",
    "print(df1.shape)\n",
    "\n",
    "# MEM INTENSIVE: remove words that only appear once across the corpus\n",
    "count_1 = sum(pd.Series(' '.join(df1[\"docl\"]).split()).value_counts()==1)\n",
    "freq = pd.Series(' '.join(df1[\"docl\"]).split()).value_counts()[-count_1:]\n",
    "freq = list(freq.index)\n",
    "df1[\"docf\"] = df1[\"docl\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "print(df1.shape)\n",
    "\n",
    "# Remove words highly common across documents\n",
    "\n",
    "freq = pd.Series(' '.join(df1[\"docl\"]).split()).value_counts()[:25]\n",
    "freq = list(freq.index)\n",
    "df1[\"docf\"] = df1[\"docf\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "# # repeat remove punctuation, leaving apostrophied possessive \n",
    "# # and hyphenated words intact; make lowercase\n",
    "# df1[\"docl\"] = df1['docl'].apply(lambda x: \" \".join(\n",
    "#     [word.strip(string.punctuation) for word in x.split(\" \")]).strip()).str.lower()\n",
    "\n",
    "df1.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_excel(\"filtered_data_\" \n",
    "             + str(datetime.datetime.now())[0:19].replace(\":\",\"_\") \n",
    "            + \".xlsx\")\n",
    "df1.to_csv(\"filtered_data_\" \n",
    "             + str(datetime.datetime.now())[0:19].replace(\":\",\"_\") \n",
    "            + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
